# Potato

Welcome to Potato! Tinkering with a Multi-Head Latent Attention (MLA) based Large Language Model (LLM).

## Whatâ€™s Inside
- **MLA Implementation**: A custom attention mechanism to save memory and boost performance.
- **LLM Setup**: A mini transformer model for text generation.
